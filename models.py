import torch
import torch.nn.functional as F

import DiffModel as Diff
import sscdr_model as SSCDR
import lacdr_model as LACDR

from rqvae import ResidualQuantizer


class LookupEmbedding(torch.nn.Module):

    def __init__(self, uid_all, iid_all, emb_dim):
        super().__init__()
        self.uid_embedding = torch.nn.Embedding(uid_all, emb_dim)
        self.iid_embedding = torch.nn.Embedding(iid_all + 1, emb_dim)

    def forward(self, x):
        uid_emb = self.uid_embedding(x[:, 0].unsqueeze(1))
        iid_emb = self.iid_embedding(x[:, 1].unsqueeze(1))
        emb = torch.cat([uid_emb, iid_emb], dim=1)
        return emb


class MFBasedModel(torch.nn.Module):
    def __init__(self, uid_all, iid_all, emb_dim, meta_dim_0, codebook_level, codebook_size):
        super().__init__()
        self.emb_dim = emb_dim
        self.src_model = LookupEmbedding(uid_all, iid_all, emb_dim)
        self.tgt_model = LookupEmbedding(uid_all, iid_all, emb_dim)
        self.aug_model = LookupEmbedding(uid_all, iid_all, emb_dim)

        self.rq = ResidualQuantizer(code_dim=emb_dim, num_levels=codebook_level, codebook_size=codebook_size)

    def forward(self, x, stage, device, diff_model=None, ss_model=None, la_model=None, is_task=False):
        if stage == "train_src":
            emb = self.src_model.forward(x)
            x = torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1)
            return x
        elif stage in ["train_tgt", "test_tgt"]:
            emb = self.tgt_model.forward(x)
            x = torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1)
            return x
        elif stage == "train_diff":
            tgt_uid, iid_input, y_input = x  # [B], [B,1], [B,1]

            # x_0 ì—­í• : íƒ€ê¹ƒ ë„ë©”ì¸ ìœ ì € ì„ë² ë”©
            tgt_emb = self.tgt_model.uid_embedding(tgt_uid.unsqueeze(1)).squeeze()  # [B, emb_dim]

            # RQ-VAE ì…ë ¥: ì†ŒìŠ¤ ë„ë©”ì¸ uid ì„ë² ë”©
            src_uid_emb = self.src_model.uid_embedding(tgt_uid.unsqueeze(1)).squeeze()  # [B, emb_dim]

            # ğŸ”¥ RQ-VAE í†µê³¼: ì½”ë“œë¶ ë ˆë²¨ ë²¡í„° í•™ìŠµ
            quantized, all_level_vectors, rq_loss = self.rq(src_uid_emb)
            # all_level_vectors: [L, B, emb_dim]  (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ diffusion condë¡œ ì“¸ ì˜ˆì •)

            iid_emb = self.tgt_model.iid_embedding(iid_input.unsqueeze(1)).squeeze()  # [B, emb_dim]

            # ì•„ì§ì€ DiffCDRëŠ” ì›ë˜ëŒ€ë¡œ src_uid_embë¥¼ cond_embë¡œ ì‚¬ìš©
            diff_loss = Diff.diffusion_loss_fn(
                diff_model,
                tgt_emb,  # x_0
                src_uid_emb,  # cond_emb (is_task=Falseì—ì„œ ì‚¬ìš©)
                iid_emb,
                y_input,
                device,
                is_task,
                all_level_vectors=all_level_vectors,  # ğŸ”¥ ì¶”ê°€
            )

            # ğŸ”¥ ì½”ë“œë¶ë„ ê°™ì´ í•™ìŠµ
            alpha_rq = 1e-2  # íŠœë‹ ê°€ëŠ¥
            total_loss = diff_loss + alpha_rq * rq_loss

            return total_loss
        elif stage == "test_diff":
            tgt_uid, iid_input, _ = x

            src_uid_emb = self.src_model.uid_embedding(tgt_uid.unsqueeze(1)).squeeze()  # [B, D]
            quantized, all_level_vectors, _ = self.rq(src_uid_emb)  # [L, B, D]

            iid_emb = self.tgt_model.iid_embedding(iid_input.unsqueeze(1)).squeeze()  # [B, D]

            # ğŸ”¥ RQ ê¸°ë°˜ ìƒ˜í”Œë§ ì‚¬ìš©
            trans_emb, iid_emb_out = Diff.p_sample_loop_with_rq(diff_model, all_level_vectors, iid_emb, device)

            x = torch.sum(trans_emb * iid_emb_out, dim=1)
            return x
